version: '3.8'

services:
  postgres:
    image: pgvector/pgvector:pg16
    container_name: postgres
    ports:
      - "127.0.0.1:${POSTGRES_PORT:-5432}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./postgres/init-pgvector.sql:/docker-entrypoint-initdb.d/init-pgvector.sql
    environment:
      - POSTGRES_DB=${POSTGRES_DB:-rag_db}
      - POSTGRES_USER=${POSTGRES_USER:-rag_user}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    restart: always
    networks:
      - rag_network

  vllm_llm:
    image: vllm/vllm-openai:latest
    container_name: vllm_llm
    ports:
      - "127.0.0.1:${VLLM_LLM_PORT:-8000}:8000"
    volumes:
      - hf_cache:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - HF_HUB_ENABLE_HF_TRANSFER=${HF_HUB_ENABLE_HF_TRANSFER:-false}
      - CUDA_LAUNCH_BLOCKING=1
      - MODEL_ID=${VLLM_LLM_MODEL_NAME}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command:
      - --model
      - ${VLLM_LLM_MODEL_NAME}
      - --max-model-len
      - ${VLLM_LLM_MAX_MODEL_LEN:-8192}
      - --gpu-memory-utilization
      - ${VLLM_LLM_GPU_MEM_UTIL:-0.9}
      - --served-model-name
      - ${VLLM_LLM_SERVED_MODEL_NAME:-vllm-llm-model}
      - --enforce-eager
      - --tensor-parallel-size
      - ${VLLM_TENSOR_PARALLEL_SIZE:-1}
    ipc: host
    restart: always
    networks:
      - rag_network

  vllm_text_embedder:
    image: vllm/vllm-openai:latest
    container_name: vllm_text_embedder
    ports:
      - "127.0.0.1:${VLLM_TEXT_EMBEDDING_PORT:-8001}:8000"
    volumes:
      - hf_cache:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - HF_HUB_ENABLE_HF_TRANSFER=${HF_HUB_ENABLE_HF_TRANSFER:-false}
      - CUDA_LAUNCH_BLOCKING=1
      - VLLM_TEXT_EMBEDDING_MODEL_NAME=${VLLM_TEXT_EMBEDDING_MODEL_NAME:-Snowflake/snowflake-arctic-embed-l-v2.0}
      - VLLM_TEXT_EMBEDDING_SERVED_NAME=${VLLM_TEXT_EMBEDDING_SERVED_NAME:-text-embedding-model}
      - VLLM_TEXT_EMBEDDING_GPU_MEM_UTIL=${VLLM_TEXT_EMBEDDING_GPU_MEM_UTIL:-0.9}
      - VLLM_TEXT_EMBEDDING_MAX_LEN=${VLLM_TEXT_EMBEDDING_MAX_LEN:-4096}
      - VLLM_TEXT_EMBEDDING_TRUST_REMOTE_CODE=${VLLM_TEXT_EMBEDDING_TRUST_REMOTE_CODE:-true}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command:
      - --model
      - ${VLLM_TEXT_EMBEDDING_MODEL_NAME}
      - --served-model-name
      - ${VLLM_TEXT_EMBEDDING_SERVED_NAME}
      - --gpu-memory-utilization
      - ${VLLM_TEXT_EMBEDDING_GPU_MEM_UTIL}
      - --max-model-len
      - ${VLLM_TEXT_EMBEDDING_MAX_LEN}
      - --tensor-parallel-size
      - ${VLLM_TENSOR_PARALLEL_SIZE:-1}
      - --task
      - embed
      - --trust-remote-code
    ipc: host
    restart: always
    networks:
      - rag_network

  vllm_image_embedder:
    image: vllm/vllm-openai:latest
    container_name: vllm_image_embedder
    ports:
      - "127.0.0.1:${VLLM_IMAGE_EMBEDDING_PORT:-8002}:8000"
    volumes:
      - hf_cache:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - HF_HUB_ENABLE_HF_TRANSFER=${HF_HUB_ENABLE_HF_TRANSFER:-false}
      - CUDA_LAUNCH_BLOCKING=1
      - VLLM_IMAGE_EMBEDDING_MODEL_NAME=${VLLM_IMAGE_EMBEDDING_MODEL_NAME}
      - VLLM_IMAGE_EMBEDDING_SERVED_NAME=${VLLM_IMAGE_EMBEDDING_SERVED_NAME:-image-embedding-model}
      - VLLM_IMAGE_EMBEDDING_GPU_MEM_UTIL=${VLLM_IMAGE_EMBEDDING_GPU_MEM_UTIL:-0.9}
      - VLLM_IMAGE_EMBEDDING_TRUST_REMOTE_CODE=${VLLM_IMAGE_EMBEDDING_TRUST_REMOTE_CODE:-false}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command:
      - --model
      - ${VLLM_IMAGE_EMBEDDING_MODEL_NAME}
      - --served-model-name
      - ${VLLM_IMAGE_EMBEDDING_SERVED_NAME}
      - --gpu-memory-utilization
      - ${VLLM_IMAGE_EMBEDDING_GPU_MEM_UTIL}
      - --tensor-parallel-size
      - ${VLLM_TENSOR_PARALLEL_SIZE:-1}
      - --task
      - embed
    ipc: host
    restart: always
    networks:
      - rag_network

  flowise:
    build:
      context: https://github.com/dentroai/Flowise.git
    container_name: flowise
    expose:
      - "3000"
    volumes:
      - flowise_data:${FLOWISE_DATABASE_PATH:-/root/.flowise}
      - flowise_data_audit:${FLOWISE_AUDIT_TRAIL_STORAGE_PATH:-/data/}
    environment:
      - PORT=3000
      - DATABASE_PATH=${FLOWISE_DATABASE_PATH:-/root/.flowise}
      - APIKEY_PATH=${FLOWISE_APIKEY_PATH:-/root/.flowise}
      - SECRETKEY_PATH=${FLOWISE_SECRETKEY_PATH:-/root/.flowise}
      - LOG_PATH=${FLOWISE_LOG_PATH:-/root/.flowise/logs}
      - BLOB_STORAGE_PATH=${FLOWISE_BLOB_STORAGE_PATH:-/root/.flowise/storage}
      - FLOWISE_AUDIT_TRAIL_STORAGE_PATH=${FLOWISE_AUDIT_TRAIL_STORAGE_PATH:-/data/}
      - DISABLE_FLOWISE_TELEMETRY=${FLOWISE_DISABLE_TELEMETRY:-true}
      - FLOWISE_USERNAME=${FLOWISE_USERNAME}
      - FLOWISE_PASSWORD=${FLOWISE_PASSWORD}
      - CORS_ORIGINS=${FLOWISE_CORS_ORIGINS}
      - IFRAME_ORIGINS=${FLOWISE_IFRAME_ORIGINS}
      - DEBUG=flowise:*
    restart: always
    networks:
      - rag_network

  aero-chat:
    build:
      context: https://x-access-token:${GITHUB_TOKEN}@github.com/${AERO_CHAT_REPO_OWNER}/${AERO_CHAT_REPO_NAME}.git#${AERO_CHAT_REPO_BRANCH}
      args:
        - NEXT_PUBLIC_MSAL_CLIENT_ID=${NEXT_PUBLIC_MSAL_CLIENT_ID}
        - NEXT_PUBLIC_MSAL_TENANT_ID=${NEXT_PUBLIC_MSAL_TENANT_ID}
        - NEXT_PUBLIC_BASE_URL=https://aero-chat.dentro-innovation.com
        - FLOWISE_API_URL=${FLOWISE_API_URL}
    container_name: aero-chat
    expose:
      - "3000"
    volumes:
      - aero_chat_data:/app/data
    environment:
      - NODE_ENV=production
      - DATABASE_URL=file:/app/data/dev.db
      - NEXT_PUBLIC_MSAL_CLIENT_ID=${NEXT_PUBLIC_MSAL_CLIENT_ID}
      - NEXT_PUBLIC_MSAL_TENANT_ID=${NEXT_PUBLIC_MSAL_TENANT_ID}
      - NEXT_PUBLIC_BASE_URL=https://aero-chat.dentro-innovation.com
      - FLOWISE_API_URL=${FLOWISE_API_URL}
    restart: always
    networks:
      - rag_network
    depends_on:
      - postgres
      - flowise

  caddy:
    image: caddy:2-alpine
    container_name: caddy
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
      - caddy_data:/data
      - caddy_config:/config
    restart: always
    networks:
      - rag_network
    depends_on:
      - aero-chat
      - flowise

volumes:
  postgres_data:
  hf_cache:
  flowise_data:
  flowise_data_audit:
  aero_chat_data:
  caddy_data:
  caddy_config:

networks:
  rag_network:
    driver: bridge 